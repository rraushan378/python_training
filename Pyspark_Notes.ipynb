{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4dIQrhFlt1O",
        "outputId": "6f685b10-04d3-4639-e50c-972e80a62c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=1c9d52fd0c3ef64e74eb8c227a4a2a47340b966b9183a25afaded83f273fde1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8H33Ktanbb6"
      },
      "source": [
        "What is SparkSession?\n",
        "\n",
        "SparkSession was introduced in version Spark 2.0, it is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame, and DataSet. SparkSession’s object spark is the default variable available in spark-shell and it can be created programmatically using SparkSession builder pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wDKj8vl-qG"
      },
      "source": [
        "sparkContext:\n",
        "\n",
        "*pyspark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variable*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5baaST1TVLu",
        "outputId": "fe10c166-980b-43c4-d5aa-c15e943e731a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Partitions: 5\n",
            "Action: First element: 1\n",
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').master(\"local[5]\").getOrCreate()\n",
        "sparkContext=spark.sparkContext\n",
        "rdd=sparkContext.parallelize([1,2,3,4,5])\n",
        "rddCollect = rdd.collect()\n",
        "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
        "print(\"Action: First element: \"+str(rdd.first()))\n",
        "print(rddCollect)\n",
        "#sparkContext.stop()\n",
        "'''you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one using stop() method'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfoR8z8AnkOr"
      },
      "source": [
        "RDD Introduction\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is a core building block of PySpark. It is a fault-tolerant, immutable, distributed collection of objects. Immutable means that once you create an RDD, you cannot change it. The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.\n",
        "\n",
        "RDD Creation\n",
        "\n",
        "Using sparkContext.parallelize()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ZHksE-Ttnc",
        "outputId": "c8c402e6-d221-48a7-e8d2-3c3f98932ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.master(\"local[1]\").getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "data=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd=sc.parallelize(data)\n",
        "print(rdd.collect())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnH5LoxQtoC-"
      },
      "source": [
        "Using File Location:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bj0bIvDTpyyb"
      },
      "outputs": [],
      "source": [
        "rdd = spark.sparkContext.textFile(\"sample_data/mnist_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fvPIxRFup9s"
      },
      "source": [
        " PySpark RDD Repartition() vs Coalesce():\n",
        "\n",
        "In PySpark, the choice between repartition() and coalesce() functions carries importance in optimizing performance and resource utilization. These methods play pivotal roles in reshuffling data across partitions within a DataFrame, yet they differ in their mechanisms and implications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKWpvTtcu36a",
        "outputId": "18d7e58a-d3e4-4dbb-cb19-d755c08a0ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From local[5] : 5\n",
            "parallelize : 6\n"
          ]
        }
      ],
      "source": [
        "# Create spark session with local[5]\n",
        "rdd = spark.sparkContext.parallelize(range(0,20))\n",
        "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
        "\n",
        "# Use parallelize with 6 partitions\n",
        "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
        "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is46WJ7OvIY7"
      },
      "source": [
        "RDD repartition():\n",
        "\n",
        "repartition() is a transformation method available on RDDs (Resilient Distributed Datasets) that redistributes data across a specified number of partitions. When you call repartition(n), where n is the desired number of partitions, Spark reshuffles the data in the RDD into exactly n partitions.\n",
        "\n",
        "* it increase or decrese the no of partition with redistribution of all data acress the specified no of partition.\n",
        "* Full shuffling is done in repartition()\n",
        "* Can be expensive for large dataset.\n",
        "* Distribute data acress partition evenly(means same size of each partition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNQKZfx4vKXc",
        "outputId": "fc232f98-e1e3-4fb6-d6fd-bf20f4b64580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repartition size : 4\n"
          ]
        }
      ],
      "source": [
        "rdd2 = rdd1.repartition(4)\n",
        "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
        "rdd2.saveAsTextFile(\"re-partition.txt\")\n",
        "\n",
        "\n",
        "'''\n",
        "Partition 1 : 1 6 10 15 19\n",
        "Partition 2 : 2 3 7 11 16\n",
        "Partition 3 : 4 8 12 13 17\n",
        "Partition 4 : 0 5 9 14 18\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUG-pznsvn0n"
      },
      "source": [
        "RDD coalesce():\n",
        "\n",
        "\n",
        "In PySpark, coalesce() is a transformation method available on RDDs (Resilient Distributed Datasets) that reduces the number of partitions without shuffling data across the cluster. When you call coalesce(n), where n is the desired number of partitions, Spark merges existing partitions to create n partitions.\n",
        "\n",
        "* It decrese the no of partition without shuffling of all data and by merging existing partition.\n",
        "* Shuffling is not done in Coalesce().\n",
        "* Less expensive then repartition().\n",
        "* Distribute data acress partition imbalance size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMRGUPzzvpPh"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.coalesce(4)\n",
        "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
        "#rdd3.saveAsTextFile(\"/tmp/coalesce\")\n",
        "\n",
        "'''\n",
        "Partition 1 : 0 1 2\n",
        "Partition 2 : 3 4 5 6 7 8 9\n",
        "Partition 4 : 10 11 12\n",
        "Partition 5 : 13 14 15 16 17 18 19\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEm2Nhr0wUSD"
      },
      "source": [
        "Broadcast Variables:\n",
        "\n",
        "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n",
        "\n",
        "\n",
        "How does PySpark Broadcast work?\n",
        "\n",
        "When you run a PySpark RDD, DataFrame applications that have the Broadcast variables defined and used, PySpark does the following.\n",
        "\n",
        "* PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
        "\n",
        "* Later Stages are also broken into tasks\n",
        "\n",
        "* Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
        "\n",
        "* The broadcasted data is cache in serialized format and deserialized before executing each task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlB6cyuwYDw",
        "outputId": "f3e10e2f-8ef5-48d6-9131-b838bee55a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ottSU_JyweCP"
      },
      "source": [
        "Accumulator:\n",
        "\n",
        "The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
        "\n",
        "\n",
        "What is PySpark Accumulator?\n",
        "\n",
        "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sVQCw6vxDG4",
        "outputId": "dbf3cd0a-1768-45d6-a53f-a837a5dccbf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
        "\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd.foreach(lambda x:accum.add(x))\n",
        "print(accum.value) #Accessed by driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ipnPbWxWLN"
      },
      "source": [
        "Create Empty RDD in PySpark:\n",
        "\n",
        "Create an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI8eptL3xOHG",
        "outputId": "abcae8b8-fed5-4813-ac22-56f280592672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EmptyRDD[32] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "#Creates Empty RDD\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "print(emptyRDD)\n",
        "\n",
        "#Diplays\n",
        "#EmptyRDD[188] at emptyRDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsEYyWJVxkKB"
      },
      "source": [
        "Create Empty DataFrame with Schema (StructType):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgDV5ixwxoL7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "schema = StructType([\n",
        "  StructField('firstname', StringType(), True),\n",
        "  StructField('middlename', StringType(), True),\n",
        "  StructField('lastname', StringType(), True)\n",
        "  ])\n",
        "\n",
        "#Create empty DataFrame from empty RDD\n",
        "df = spark.createDataFrame(emptyRDD,schema)\n",
        "df.printSchema()\n",
        "#convert RDD to DF\n",
        "df1 = emptyRDD.toDF(schema)\n",
        "df1.printSchema()\n",
        "\n",
        "#Create empty DataFrame directly.\n",
        "df2 = spark.createDataFrame([], schema)\n",
        "df2.printSchema()\n",
        "\n",
        "\n",
        "#Create empty DatFrame with no schema (no columns)\n",
        "df3 = spark.createDataFrame([], StructType([]))\n",
        "df3.printSchema()\n",
        "\n",
        "#print below empty schema\n",
        "#root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TaL6UltyFYQ"
      },
      "source": [
        "Convert RDD to DF:\n",
        "\n",
        "1.Using rdd.toDF() function\n",
        "PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n",
        "\n",
        "2.Using PySpark createDataFrame() function\n",
        "\n",
        "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re6Kco80yIS2",
        "outputId": "ccbe3dd4-7fa1-45b8-b8e8-234debd9252d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)\n",
        "#using toDF function\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "# with selected columns\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "#using createDataFrame function\n",
        "'''deptSchema = StructType([\n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])'''\n",
        "df3 = spark.createDataFrame(rdd,[\"dept_name\",\"dept_id\"]) #or (rdd,schema=deptSchema)\n",
        "df3.printSchema()\n",
        "df3.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJkP-fHoy7Jh"
      },
      "source": [
        "Convert PySpark Dataframe to Pandas DataFrame\n",
        "\n",
        "PySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame.\n",
        "\n",
        "toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done only on a small subset of the data. running on larger dataset’s results in memory error and crashes the application. To deal with a larger dataset, you can also try increasing memory on the driver.\n",
        "\n",
        "StructType – Defines the structure of the DataFrame\n",
        "\n",
        "StructField – Defines the metadata of the DataFrame column\n",
        "\n",
        "It represents a field in the schema, containing metadata such as the name, data type, and nullable status of the field. Each StructField object defines a single column in the DataFrame, specifying its name and the type of data it holds.\n",
        "You can Check Below Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZGyIPOZlz6ID"
      },
      "outputs": [],
      "source": [
        "# Nested structure elements\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
        "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
        "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
        "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
        "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
        "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
        "]\n",
        "\n",
        "schemaStruct = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\\\n",
        "             StructField('middlename', StringType(), True),\\\n",
        "             StructField('lastname', StringType(), True)\\\n",
        "             ])),\\\n",
        "          StructField('dob', StringType(), True),\\\n",
        "         StructField('gender', StringType(), True),\\\n",
        "         StructField('salary', StringType(), True)\\\n",
        "         ])\n",
        "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
        "df.printSchema()\n",
        "#using toPandas()\n",
        "pandasDF2 = df.toPandas()\n",
        "print(pandasDF2)\n",
        "\n",
        "# Default - displays 20 rows and\n",
        "# 20 charactes from column value\n",
        "df.show()\n",
        "\n",
        "#Display full column contents\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Display 2 rows and full column contents\n",
        "df.show(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF9gHXqM2mkT"
      },
      "outputs": [],
      "source": [
        "#Create DataFrame with struct using Row class\n",
        "from pyspark.sql import Row\n",
        "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
        "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
        "df=spark.createDataFrame(data)\n",
        "df.printSchema()\n",
        "#root\n",
        "# |-- name: string (nullable = true)\n",
        "# |-- prop: struct (nullable = true)\n",
        "# |    |-- hair: string (nullable = true)\n",
        "# |    |-- eye: string (nullable = true)\n",
        "\n",
        "#Access struct column\n",
        "df.select(df.prop.hair).show()\n",
        "df.select(df[\"prop.hair\"]).show()\n",
        "df.select(col(\"prop.hair\")).show()\n",
        "\n",
        "#Access all columns from struct\n",
        "df.select(col(\"prop.*\")).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8x1dt7R23Jb"
      },
      "source": [
        "Column Functions:\n",
        "\n",
        "Arithmetic(),\n",
        "alias(),\n",
        "isin(),\n",
        "asc(),\n",
        "desc(),\n",
        "contains(),\n",
        "between(),\n",
        "cast(),\n",
        "like(),\n",
        "substring(),\n",
        "when() & otherwise()\n",
        "\n",
        "**select**() function:\n",
        "\n",
        " It is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns.\n",
        "\n",
        " Collect():\n",
        "\n",
        " collect() function of the RDD/DataFrame is an action operation that returns all elements of the DataFrame\n",
        "\n",
        "\n",
        "collect () vs select ()\n",
        "\n",
        "select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmRx0PcS26La"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "#Arthmetic operations\n",
        "df.select(df.col1 + df.col2).show()\n",
        "'''df.select(df.col1 - df.col2).show()\n",
        "df.select(df.col1 * df.col2).show()\n",
        "df.select(df.col1 / df.col2).show()\n",
        "df.select(df.col1 % df.col2).show()\n",
        "df.select(df.col2 > df.col3).show()\n",
        "df.select(df.col2 < df.col3).show()\n",
        "df.select(df.col2 == df.col3).show()'''\n",
        "\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "#show with alias() function\n",
        "df.select(df.fname.alias(\"first_name\"), \\\n",
        "          df.lname.alias(\"last_name\")\n",
        "   ).show(1)\n",
        "\n",
        "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\")).show(1) #import expr\n",
        "#asc, desc to sort ascending and descending order repsectively.\n",
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()\n",
        "df.sort(\"fname\", \"lname\", ascending=[True, False]) \\\n",
        "  .show()\n",
        "#print('Ordery function')\n",
        "df.orderBy(col(\"fname\").asc(),col(\"lname\").asc()).show(truncate=False)\n",
        "\n",
        "print('Sort using spark ')\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select fname,lname,id,gender from EMP ORDER BY fname asc\").show(truncate=False)\n",
        "\n",
        "\n",
        "#cast\n",
        "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n",
        "#between\n",
        "df.filter(df.id.between(100,300)).show()\n",
        "#contains\n",
        "df.filter(df.fname.contains(\"Cruise\")).show()\n",
        "#startswith, endswith()\n",
        "df.filter(df.fname.startswith(\"T\")).show()\n",
        "df.filter(df.fname.endswith(\"Cruise\")).show()\n",
        "#isNull & isNotNull\n",
        "df.filter(df.lname.isNull()).show()\n",
        "df.filter(df.lname.isNotNull()).show()\n",
        "#like , rlike\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.fname.like(\"%om\"))\n",
        "#Substring\n",
        "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n",
        "#when & otherwise\n",
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
        "              .when(df.gender==\"F\",\"Female\") \\\n",
        "              .when(df.gender==None ,\"\") \\\n",
        "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
        "    ).show()\n",
        "#isin\n",
        "li=[\"100\",\"200\"]\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.id.isin(li)) \\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF9JB_TN7To0"
      },
      "source": [
        "withColumn():\n",
        "\n",
        " It is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
        "\n",
        "\n",
        " What is the difference between where and filter in PySpark?\n",
        "\n",
        "In PySpark, both filter() and where() functions are used to select out data based on certain conditions. They are used interchangeably, and both of them essentially perform the same operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQe7QxUT7dNO",
        "outputId": "9813e32c-2720-4aac-8d25-2f45e3869edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----------+\n",
            "|gender|sum(salary)|\n",
            "+------+-----------+\n",
            "|M     |11000      |\n",
            "|F     |3999       |\n",
            "+------+-----------+\n",
            "\n",
            "+------+----------+------------------+-------+-------+\n",
            "|gender|sum_salary|avg_salary        |sum_sal|max_sal|\n",
            "+------+----------+------------------+-------+-------+\n",
            "|M     |11000     |3666.6666666666665|11000  |4000   |\n",
            "|F     |3999      |1999.5            |3999   |4000   |\n",
            "+------+----------+------------------+-------+-------+\n",
            "\n",
            "+------+----------+------------------+-------+-------+\n",
            "|gender|sum_salary|        avg_salary|sum_sal|max_sal|\n",
            "+------+----------+------------------+-------+-------+\n",
            "|     M|     11000|3666.6666666666665|  11000|   4000|\n",
            "|     F|      3999|            1999.5|   3999|   4000|\n",
            "+------+----------+------------------+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr,col\n",
        "from pyspark.sql.functions import sum,avg,max\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()\n",
        "#Update The Value of an Existing Column\n",
        "df.withColumn(\"salary\",col(\"salary\")*100).show()\n",
        "#Create a Column from an Existing\n",
        "df.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n",
        "#Rename Column\n",
        "df.withColumnRenamed(\"gender\",\"sex\") \\\n",
        "  .show(truncate=False)\n",
        "#drop column\n",
        "df.drop(\"salary\") \\\n",
        "  .show()\n",
        "# Remove duplicates on selected columns using dropDuplicates()\n",
        "dropDisDF = df.dropDuplicates([\"gender\",\"salary\"])\n",
        "print(\"Distinct count of gender & salary : \"+str(dropDisDF.count()))\n",
        "dropDisDF.show(truncate=False)\n",
        "\n",
        "# Sort using spark SQL\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n",
        "\n",
        "\n",
        "#filter\n",
        "# Using equal condition\n",
        "df.filter(df.salary == 3000).show(truncate=False)\n",
        "# Using SQL Expression\n",
        "df.filter(\"gender == 'M'\").show()\n",
        "# Filter multiple conditions\n",
        "df.filter( (df.lastname  == \"Smith\") & (df.gender  == \"M\") ) \\\n",
        "    .show(truncate=False)\n",
        "df.filter(df.firstname.like(\"%ber%\")).show()\n",
        "\n",
        "# Using groupBy().sum()\n",
        "df.groupBy(\"gender\").sum(\"salary\").show(truncate=False)\n",
        "# Using filter on aggregate data\n",
        "df.groupBy(\"gender\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "      sum(\"salary\").alias(\"sum_sal\"), \\\n",
        "      max(\"salary\").alias(\"max_sal\")) \\\n",
        "    .where(col(\"sum_salary\") >= 500) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "# Register DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# Using SQL Query\n",
        "sql_string = \"\"\"SELECT gender,\n",
        "       SUM(salary) AS sum_salary,\n",
        "       AVG(salary) AS avg_salary,\n",
        "       SUM(salary) AS sum_sal,\n",
        "       MAX(salary) AS max_sal\n",
        "FROM employees\n",
        "GROUP BY gender\n",
        "HAVING SUM(salary) >= 1000\"\"\"\n",
        "\n",
        "# Execute SQL query against the temporary view\n",
        "df2 = spark.sql(sql_string)\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfNvet2HIL1"
      },
      "source": [
        "JOIN():\n",
        "\n",
        "How Join works?\n",
        "\n",
        "PySpark’s join operation combines data from two or more Datasets based on a common column or key. It is a fundamental operation in PySpark and is similar to SQL joins.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl14tB5x_GmM"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.createOrReplaceTempView(\"EMP\")\n",
        "deptDF.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "joinDF2 = spark.sql(\"select * from EMP e Left JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyA4C3B4JqVz"
      },
      "source": [
        "Map() & FlatMap()\n",
        "\n",
        "The map()in PySpark is a transformation function that is used to apply a function/lambda to each element of an RDD (Resilient Distributed Dataset) and return a new RDD consisting of the result.\n",
        "\n",
        "\n",
        "PySpark flatMap(): is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndCfpNj8JssW",
        "outputId": "3c5ce051-08ed-42e9-9841-a05b03f65e1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Alice’s', 1),\n",
              " ('Gutenberg’s', 3),\n",
              " ('Adventures', 2),\n",
              " ('in', 2),\n",
              " ('Wonderland', 2),\n",
              " ('Project', 3)]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "# map() with rdd\n",
        "rdd2=rdd.map(lambda x: (x,1))\n",
        "#for element in rdd2.collect():\n",
        " #   print(element)\n",
        "rdd3=rdd2.reduceByKey(lambda x,y:x+y)\n",
        "rdd3.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tr5avwK3xy",
        "outputId": "3e28c98e-5ed7-45bb-84f4-fed432e944af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "data = [\"Project Gutenberg’s\",\n",
        "        \"Alice’s Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\",\n",
        "        \"Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "#for element in rdd.collect():\n",
        "  #  print(element)\n",
        "\n",
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axY2i-NHL6Ic"
      },
      "source": [
        "EXPLODE Function():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDRi8CgbL4ov"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
        "\n",
        "arrayData = [\n",
        "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
        "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
        "        ('Washington',None,None),\n",
        "        ('Jefferson',['1','2'],{})]\n",
        "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "df2 = df.select(df.name,explode(df.knownLanguages))\n",
        "df2.printSchema()\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuA5PH4WMb2i",
        "outputId": "23ceda3d-b337-4925-d8c9-353df33be5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "|Seqno|        Name|\n",
            "+-----+------------+\n",
            "|    1|  john jones|\n",
            "|    2|tracey smith|\n",
            "|    3| amy sanders|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "\n",
        "# foreach() Example\n",
        "def f(df):\n",
        "    print(df.Seqno)\n",
        "df.foreach(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih2C5UpVNEtz"
      },
      "source": [
        "Read Files csv, parquet, Excel, text etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyGLwITHNLzT"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read CSV File\n",
        "#df = spark.read.csv(\"sample_data/mnist_test.csv\")\n",
        "#spark.write.csv(\"sample_data/mnist_test1.csv\")\n",
        "#df3 = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
        " # .csv(\"sample_data/mnist_test.csv\")\n",
        "df4 = spark.read.json(\"sample_data/anscombe.json\")\n",
        "df4.printSchema()\n",
        "df4.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
