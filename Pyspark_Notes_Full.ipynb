{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4dIQrhFlt1O",
        "outputId": "2f38eea1-94d7-404f-88f3-0aee1746280c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is SparkSession?\n",
        "\n",
        "SparkSession was introduced in version Spark 2.0, it is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame, and DataSet. SparkSession’s object spark is the default variable available in spark-shell and it can be created programmatically using SparkSession builder pattern."
      ],
      "metadata": {
        "id": "M8H33Ktanbb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sparkContext:\n",
        "\n",
        "*pyspark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variable*\n"
      ],
      "metadata": {
        "id": "W9wDKj8vl-qG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5baaST1TVLu",
        "outputId": "fe10c166-980b-43c4-d5aa-c15e943e731a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Partitions: 5\n",
            "Action: First element: 1\n",
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').master(\"local[5]\").getOrCreate()\n",
        "sparkContext=spark.sparkContext\n",
        "rdd=sparkContext.parallelize([1,2,3,4,5])\n",
        "rddCollect = rdd.collect()\n",
        "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
        "print(\"Action: First element: \"+str(rdd.first()))\n",
        "print(rddCollect)\n",
        "#sparkContext.stop()\n",
        "'''you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one using stop() method'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD Introduction\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is a core building block of PySpark. It is a fault-tolerant, immutable, distributed collection of objects. Immutable means that once you create an RDD, you cannot change it. The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.\n",
        "\n",
        "* it is a collection of rows without schema\n",
        "* it is immutable\n",
        "* it is lazy execuation at row level and eager execution at schema level\n",
        "\n",
        "\n",
        "**Spark DataFrame**\n",
        "\n",
        "In Spark Scala, a DataFrame is a distributed collection of data organized into named columns similar to an SQL table.\n",
        "\n",
        "* It is similar to a table in a relational database or a spreadsheet in that it has a schema, which defines the types and names of its columns, and each row represents a single record or observation.\n",
        "* DataFrames in Spark Scala can be created from a variety of sources, such as RDDs, structured data files (e.g., CSV, JSON, Parquet), Hive tables, or external databases\n",
        "* Once created, DataFrames support a wide range of operations and transformations, such as filtering, aggregating, joining, and grouping data.\n",
        "* One of the key benefits of using DataFrames in Spark Scala is their ability to leverage Spark’s distributed computing capabilities to process large amounts of data quickly and efficiently.\n",
        "\n",
        "\n",
        "RDD Creation\n",
        "\n",
        "Using sparkContext.parallelize()\n",
        "\n"
      ],
      "metadata": {
        "id": "sfoR8z8AnkOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.master(\"local[1]\").getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "data=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd=sc.parallelize(data)\n",
        "print(rdd.collect())\n"
      ],
      "metadata": {
        "id": "u3ZHksE-Ttnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c402e6-d221-48a7-e8d2-3c3f98932ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using File Location:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnH5LoxQtoC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile(\"sample_data/mnist_test.csv\")\n",
        "print(rdd.count())"
      ],
      "metadata": {
        "id": "bj0bIvDTpyyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c6af1d-96f0-4054-dccc-9b00cfed622e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=spark.read.csv(path=\"sample_data/mnist_test.csv\",header=True,inferSchema=True)\n",
        "rdd1.show(1)\n",
        "rdd2=spark.read.json(path=\"sample_data/anscombe.json\")\n",
        "rdd2.show()\n",
        "# Example of using option()\n",
        "#default is patquet\n",
        "rdd2.write.format(\"csv\")  \\\n",
        "        .option(\"header\", \"true\")  \\\n",
        "        .option(\"delimiter\", \"|\")  \\\n",
        "        .save(\"sample_data/output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxrluZ8UPnm4",
        "outputId": "56c0f625-9033-4771-84fd-b0a445f2c71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "|     I| 7.0| 4.81|           NULL|\n",
            "|     I| 5.0| 5.68|           NULL|\n",
            "|    II|10.0| 9.14|           NULL|\n",
            "|    II| 8.0| 8.14|           NULL|\n",
            "|    II|13.0| 8.74|           NULL|\n",
            "|    II| 9.0| 8.77|           NULL|\n",
            "|    II|11.0| 9.26|           NULL|\n",
            "|    II|14.0|  8.1|           NULL|\n",
            "|    II| 6.0| 6.13|           NULL|\n",
            "|    II| 4.0|  3.1|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " PySpark RDD Repartition() vs Coalesce():\n",
        "\n",
        "In PySpark, the choice between repartition() and coalesce() functions carries importance in optimizing performance and resource utilization. These methods play pivotal roles in reshuffling data across partitions within a DataFrame, yet they differ in their mechanisms and implications.\n"
      ],
      "metadata": {
        "id": "1fvPIxRFup9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark session with local[5]\n",
        "rdd = spark.sparkContext.parallelize(range(0,20))\n",
        "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
        "\n",
        "# Use parallelize with 6 partitions\n",
        "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
        "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKWpvTtcu36a",
        "outputId": "18d7e58a-d3e4-4dbb-cb19-d755c08a0ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From local[5] : 5\n",
            "parallelize : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD repartition():\n",
        "\n",
        "repartition() is a transformation method available on RDDs (Resilient Distributed Datasets) that redistributes data across a specified number of partitions. When you call repartition(n), where n is the desired number of partitions, Spark reshuffles the data in the RDD into exactly n partitions.\n",
        "\n",
        "\n",
        "\n",
        "* it increase or decrese the no of partition with redistribution of all data acress the specified no of partition.\n",
        "* Full shuffling is done in repartition()\n",
        "* Can be expensive for large dataset.\n",
        "* Distribute data acress partition evenly(means same size of each partition)"
      ],
      "metadata": {
        "id": "is46WJ7OvIY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = rdd1.repartition(4)\n",
        "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
        "rdd2.saveAsTextFile(\"re-partition.txt\")\n",
        "\n",
        "'''\n",
        "Partition 1 : 1 6 10 15 19\n",
        "Partition 2 : 2 3 7 11 16\n",
        "Partition 3 : 4 8 12 13 17\n",
        "Partition 4 : 0 5 9 14 18\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNQKZfx4vKXc",
        "outputId": "fc232f98-e1e3-4fb6-d6fd-bf20f4b64580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repartition size : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD coalesce():\n",
        "\n",
        "\n",
        "In PySpark, coalesce() is a transformation method available on RDDs (Resilient Distributed Datasets) that reduces the number of partitions without shuffling data across the cluster. When you call coalesce(n), where n is the desired number of partitions, Spark merges existing partitions to create n partitions.\n",
        "\n",
        "* It decrese the no of partition without shuffling of all data and by merging existing partition.\n",
        "* Shuffling is not done in Coalesce().\n",
        "* Less expensive then repartition().\n",
        "* Distribute data acress partition imbalance size."
      ],
      "metadata": {
        "id": "PUG-pznsvn0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = rdd1.coalesce(4)\n",
        "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
        "#rdd3.saveAsTextFile(\"/tmp/coalesce\")\n",
        "\n",
        "'''Partition 1 : 0 1 2\n",
        "Partition 2 : 3 4 5 6 7 8 9\n",
        "Partition 4 : 10 11 12\n",
        "Partition 5 : 13 14 15 16 17 18 19'''"
      ],
      "metadata": {
        "id": "OMRGUPzzvpPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcast Variables:\n",
        "\n",
        "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n",
        "\n",
        "\n",
        "How does PySpark Broadcast work?\n",
        "\n",
        "When you run a PySpark RDD, DataFrame applications that have the Broadcast variables defined and used, PySpark does the following.\n",
        "\n",
        "*PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
        "\n",
        "*Later Stages are also broken into tasks\n",
        "\n",
        "*Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
        "\n",
        "*The broadcasted data is cache in serialized format and deserialized before executing each task."
      ],
      "metadata": {
        "id": "XEm2Nhr0wUSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlB6cyuwYDw",
        "outputId": "f3e10e2f-8ef5-48d6-9131-b838bee55a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accumulator:\n",
        "\n",
        "The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
        "\n",
        "\n",
        "What is PySpark Accumulator?\n",
        "\n",
        "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property"
      ],
      "metadata": {
        "id": "ottSU_JyweCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
        "\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd.foreach(lambda x:accum.add(x))\n",
        "print(accum.value) #Accessed by driver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sVQCw6vxDG4",
        "outputId": "dbf3cd0a-1768-45d6-a53f-a837a5dccbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Empty RDD in PySpark:\n",
        "\n",
        "Create an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD()."
      ],
      "metadata": {
        "id": "35ipnPbWxWLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "#Creates Empty RDD\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "print(emptyRDD)\n",
        "\n",
        "#Diplays\n",
        "#EmptyRDD[188] at emptyRDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI8eptL3xOHG",
        "outputId": "abcae8b8-fed5-4813-ac22-56f280592672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmptyRDD[32] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Empty DataFrame with Schema (StructType):\n",
        "\n"
      ],
      "metadata": {
        "id": "qsEYyWJVxkKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "schema = StructType([\n",
        "  StructField('firstname', StringType(), True),\n",
        "  StructField('middlename', StringType(), True),\n",
        "  StructField('lastname', StringType(), True)\n",
        "  ])\n",
        "\n",
        "#Create empty DataFrame from empty RDD\n",
        "df = spark.createDataFrame(emptyRDD,schema)\n",
        "df.printSchema()\n",
        "#convert RDD to DF\n",
        "df1 = emptyRDD.toDF(schema)\n",
        "df1.printSchema()\n",
        "\n",
        "#Create empty DataFrame directly.\n",
        "df2 = spark.createDataFrame([], schema)\n",
        "df2.printSchema()\n",
        "\n",
        "\n",
        "#Create empty DatFrame with no schema (no columns)\n",
        "df3 = spark.createDataFrame([], StructType([]))\n",
        "df3.printSchema()\n",
        "\n",
        "#print below empty schema\n",
        "#root"
      ],
      "metadata": {
        "id": "jgDV5ixwxoL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert RDD to DF:\n",
        "\n",
        "1.Using rdd.toDF() function\n",
        "PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n",
        "\n",
        "2.Using PySpark createDataFrame() function\n",
        "\n",
        "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument."
      ],
      "metadata": {
        "id": "1TaL6UltyFYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)\n",
        "#using toDF function\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "# with selected columns\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "#using createDataFrame function\n",
        "'''deptSchema = StructType([\n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])'''\n",
        "df3 = spark.createDataFrame(rdd,[\"dept_name\",\"dept_id\"]) #or (rdd,schema=deptSchema)\n",
        "df3.printSchema()\n",
        "df3.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re6Kco80yIS2",
        "outputId": "ccbe3dd4-7fa1-45b8-b8e8-234debd9252d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert PySpark Dataframe to Pandas DataFrame\n",
        "\n",
        "PySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame.\n",
        "\n",
        "toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done only on a small subset of the data. running on larger dataset’s results in memory error and crashes the application. To deal with a larger dataset, you can also try increasing memory on the driver.\n",
        "\n",
        "StructType – Defines the structure of the DataFrame\n",
        "\n",
        "StructField – Defines the metadata of the DataFrame column\n",
        "\n",
        "It represents a field in the schema, containing metadata such as the name, data type, and nullable status of the field. Each StructField object defines a single column in the DataFrame, specifying its name and the type of data it holds.\n",
        "You can Check Below Example\n"
      ],
      "metadata": {
        "id": "dJkP-fHoy7Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested structure elements\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
        "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
        "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
        "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
        "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
        "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
        "]\n",
        "\n",
        "schemaStruct = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\\\n",
        "             StructField('middlename', StringType(), True),\\\n",
        "             StructField('lastname', StringType(), True)\\\n",
        "             ])),\\\n",
        "          StructField('dob', StringType(), True),\\\n",
        "         StructField('gender', StringType(), True),\\\n",
        "         StructField('salary', StringType(), True)\\\n",
        "         ])\n",
        "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
        "df.printSchema()\n",
        "#using toPandas()\n",
        "pandasDF2 = df.toPandas()\n",
        "print(pandasDF2)\n",
        "\n",
        "# Default - displays 20 rows and\n",
        "# 20 charactes from column value\n",
        "df.show()\n",
        "\n",
        "#Display full column contents\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Display 2 rows and full column contents\n",
        "df.show(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)"
      ],
      "metadata": {
        "id": "ZGyIPOZlz6ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create DataFrame with struct using Row class\n",
        "from pyspark.sql import Row\n",
        "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
        "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
        "df=spark.createDataFrame(data)\n",
        "df.printSchema()\n",
        "#root\n",
        "# |-- name: string (nullable = true)\n",
        "# |-- prop: struct (nullable = true)\n",
        "# |    |-- hair: string (nullable = true)\n",
        "# |    |-- eye: string (nullable = true)\n",
        "\n",
        "#Access struct column\n",
        "df.select(df.prop.hair).show()\n",
        "df.select(df[\"prop.hair\"]).show()\n",
        "df.select(col(\"prop.hair\")).show()\n",
        "\n",
        "#Access all columns from struct\n",
        "df.select(col(\"prop.*\")).show()\n"
      ],
      "metadata": {
        "id": "yF9gHXqM2mkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Column Functions:\n",
        "\n",
        "Arithmetic(),\n",
        "alias(),\n",
        "isin(),\n",
        "asc(),\n",
        "desc(),\n",
        "contains(),\n",
        "between(),\n",
        "cast(),\n",
        "like(),\n",
        "substring(),\n",
        "when() & otherwise()\n",
        "\n",
        "**select**() function:\n",
        "\n",
        " It is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns.\n",
        "\n",
        " Collect():\n",
        "\n",
        " collect() function of the RDD/DataFrame is an action operation that returns all elements of the DataFrame\n",
        "\n",
        "\n",
        "collect () vs select ()\n",
        "\n",
        "select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
      ],
      "metadata": {
        "id": "z8x1dt7R23Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "#Arthmetic operations\n",
        "df.select(df.col1 + df.col2).show()\n",
        "'''df.select(df.col1 - df.col2).show()\n",
        "df.select(df.col1 * df.col2).show()\n",
        "df.select(df.col1 / df.col2).show()\n",
        "df.select(df.col1 % df.col2).show()\n",
        "df.select(df.col2 > df.col3).show()\n",
        "df.select(df.col2 < df.col3).show()\n",
        "df.select(df.col2 == df.col3).show()'''\n",
        "\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "#show with alias() function\n",
        "df.select(df.fname.alias(\"first_name\"), \\\n",
        "          df.lname.alias(\"last_name\")\n",
        "   ).show(1)\n",
        "\n",
        "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\")).show(1) #import expr\n",
        "#asc, desc to sort ascending and descending order repsectively.\n",
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()\n",
        "df.sort(\"fname\", \"lname\", ascending=[True, False]) \\\n",
        "  .show()\n",
        "#print('Ordery function')\n",
        "df.orderBy(col(\"fname\").asc(),col(\"lname\").asc()).show(truncate=False)\n",
        "\n",
        "print('Sort using spark ')\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select fname,lname,id,gender from EMP ORDER BY fname asc\").show(truncate=False)\n",
        "\n",
        "\n",
        "#cast\n",
        "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n",
        "#between\n",
        "df.filter(df.id.between(100,300)).show()\n",
        "#contains\n",
        "df.filter(df.fname.contains(\"Cruise\")).show()\n",
        "#startswith, endswith()\n",
        "df.filter(df.fname.startswith(\"T\")).show()\n",
        "df.filter(df.fname.endswith(\"Cruise\")).show()\n",
        "#isNull & isNotNull\n",
        "df.filter(df.lname.isNull()).show()\n",
        "df.filter(df.lname.isNotNull()).show()\n",
        "#like , rlike\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.fname.like(\"%om\"))\n",
        "#Substring\n",
        "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n",
        "#when & otherwise\n",
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
        "              .when(df.gender==\"F\",\"Female\") \\\n",
        "              .when(df.gender==None ,\"\") \\\n",
        "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
        "    ).show()\n",
        "#isin\n",
        "li=[\"100\",\"200\"]\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.id.isin(li)) \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "dmRx0PcS26La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "withColumn():\n",
        "\n",
        " It is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
        "\n",
        "\n",
        " What is the difference between where and filter in PySpark?\n",
        "\n",
        "In PySpark, both filter() and where() functions are used to select out data based on certain conditions. They are used interchangeably, and both of them essentially perform the same operation."
      ],
      "metadata": {
        "id": "WF9JB_TN7To0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr,col\n",
        "from pyspark.sql.functions import sum,avg,max\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()\n",
        "#Update The Value of an Existing Column\n",
        "df.withColumn(\"salary\",col(\"salary\")*100).show()\n",
        "#Create a Column from an Existing\n",
        "df.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n",
        "#Rename Column\n",
        "df.withColumnRenamed(\"gender\",\"sex\") \\\n",
        "  .show(truncate=False)\n",
        "#drop column\n",
        "df.drop(\"salary\") \\\n",
        "  .show()\n",
        "# Remove duplicates on selected columns using dropDuplicates()\n",
        "dropDisDF = df.dropDuplicates([\"gender\",\"salary\"])\n",
        "print(\"Distinct count of gender & salary : \"+str(dropDisDF.count()))\n",
        "dropDisDF.show(truncate=False)\n",
        "\n",
        "# Sort using spark SQL\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n",
        "\n",
        "\n",
        "#filter\n",
        "# Using equal condition\n",
        "df.filter(df.salary == 3000).show(truncate=False)\n",
        "# Using SQL Expression\n",
        "df.filter(\"gender == 'M'\").show()\n",
        "# Filter multiple conditions\n",
        "df.filter( (df.lastname  == \"Smith\") & (df.gender  == \"M\") ) \\\n",
        "    .show(truncate=False)\n",
        "df.filter(df.firstname.like(\"%ber%\")).show()\n",
        "\n",
        "# Using groupBy().sum()\n",
        "df.groupBy(\"gender\").sum(\"salary\").show(truncate=False)\n",
        "# Using filter on aggregate data\n",
        "df.groupBy(\"gender\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "      sum(\"salary\").alias(\"sum_sal\"), \\\n",
        "      max(\"salary\").alias(\"max_sal\")) \\\n",
        "    .where(col(\"sum_salary\") >= 500) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "# Register DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# Using SQL Query\n",
        "sql_string = \"\"\"SELECT gender,\n",
        "       SUM(salary) AS sum_salary,\n",
        "       AVG(salary) AS avg_salary,\n",
        "       SUM(salary) AS sum_sal,\n",
        "       MAX(salary) AS max_sal\n",
        "FROM employees\n",
        "GROUP BY gender\n",
        "HAVING SUM(salary) >= 1000\"\"\"\n",
        "\n",
        "# Execute SQL query against the temporary view\n",
        "df2 = spark.sql(sql_string)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQe7QxUT7dNO",
        "outputId": "9813e32c-2720-4aac-8d25-2f45e3869edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|gender|sum(salary)|\n",
            "+------+-----------+\n",
            "|M     |11000      |\n",
            "|F     |3999       |\n",
            "+------+-----------+\n",
            "\n",
            "+------+----------+------------------+-------+-------+\n",
            "|gender|sum_salary|avg_salary        |sum_sal|max_sal|\n",
            "+------+----------+------------------+-------+-------+\n",
            "|M     |11000     |3666.6666666666665|11000  |4000   |\n",
            "|F     |3999      |1999.5            |3999   |4000   |\n",
            "+------+----------+------------------+-------+-------+\n",
            "\n",
            "+------+----------+------------------+-------+-------+\n",
            "|gender|sum_salary|        avg_salary|sum_sal|max_sal|\n",
            "+------+----------+------------------+-------+-------+\n",
            "|     M|     11000|3666.6666666666665|  11000|   4000|\n",
            "|     F|      3999|            1999.5|   3999|   4000|\n",
            "+------+----------+------------------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JOIN():\n",
        "\n",
        "How Join works?\n",
        "\n",
        "PySpark’s join operation combines data from two or more Datasets based on a common column or key. It is a fundamental operation in PySpark and is similar to SQL joins.\n"
      ],
      "metadata": {
        "id": "BLfNvet2HIL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.createOrReplaceTempView(\"EMP\")\n",
        "deptDF.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "joinDF2 = spark.sql(\"select * from EMP e Left JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "id": "Bl14tB5x_GmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map() & FlatMap()\n",
        "\n",
        "The map()in PySpark is a transformation function that is used to apply a function/lambda to each element of an RDD (Resilient Distributed Dataset) and return a new RDD consisting of the result.\n",
        "\n",
        "\n",
        "PySpark flatMap(): is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame.\n"
      ],
      "metadata": {
        "id": "XyA4C3B4JqVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "# map() with rdd\n",
        "rdd2=rdd.map(lambda x: (x,1))\n",
        "#for element in rdd2.collect():\n",
        " #   print(element)\n",
        "rdd3=rdd2.reduceByKey(lambda x,y:x+y)\n",
        "rdd3.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndCfpNj8JssW",
        "outputId": "3c5ce051-08ed-42e9-9841-a05b03f65e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Alice’s', 1),\n",
              " ('Gutenberg’s', 3),\n",
              " ('Adventures', 2),\n",
              " ('in', 2),\n",
              " ('Wonderland', 2),\n",
              " ('Project', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "data = [\"Project Gutenberg’s\",\n",
        "        \"Alice’s Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\",\n",
        "        \"Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "#for element in rdd.collect():\n",
        "  #  print(element)\n",
        "\n",
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tr5avwK3xy",
        "outputId": "3e28c98e-5ed7-45bb-84f4-fed432e944af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLODE Function():"
      ],
      "metadata": {
        "id": "axY2i-NHL6Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
        "\n",
        "arrayData = [\n",
        "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
        "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
        "        ('Washington',None,None),\n",
        "        ('Jefferson',['1','2'],{})]\n",
        "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "df2 = df.select(df.name,explode(df.knownLanguages))\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "WDRi8CgbL4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "\n",
        "# foreach() Example\n",
        "def f(df):\n",
        "    print(df.Seqno)\n",
        "df.foreach(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuA5PH4WMb2i",
        "outputId": "23ceda3d-b337-4925-d8c9-353df33be5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|        Name|\n",
            "+-----+------------+\n",
            "|    1|  john jones|\n",
            "|    2|tracey smith|\n",
            "|    3| amy sanders|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Files csv, parquet, Excel, text etc"
      ],
      "metadata": {
        "id": "Ih2C5UpVNEtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read CSV File\n",
        "#df = spark.read.csv(\"sample_data/mnist_test.csv\")\n",
        "#spark.write.csv(\"sample_data/mnist_test1.csv\")\n",
        "#df3 = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
        " # .csv(\"sample_data/mnist_test.csv\")\n",
        "df4 = spark.read.json(\"sample_data/anscombe.json\")\n",
        "df4.printSchema()\n",
        "df4.show()"
      ],
      "metadata": {
        "id": "KyGLwITHNLzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****BroadCast Join*****\n",
        "\n",
        "Broadcast join is an optimization technique in the Spark SQL engine that is used to join two DataFrames. This technique is ideal for joining a large DataFrame with a smaller one. Traditional joins take longer as they require more data shuffling and data is always collected at the driver.\n",
        "\n",
        "* The primary goal of a broadcast join is to eliminate data shuffling and network overhead associated with join operations, which can result in considerable speed benefits.\n",
        "\n",
        "* A broadcast join sends the smaller table (or DataFrame) to all worker nodes, ensuring each worker node has a complete copy of the smaller table in memory.\n",
        "\n",
        "**Types of Broadcast join.**\n",
        "\n",
        "There are two types of broadcast joins.\n",
        "\n",
        "* Broadcast hash joins:     \n",
        "In this case, the driver builds the in-memory hash DataFrame to distribute it to the executors.\n",
        "* Broadcast nested loop join:    \n",
        "It is a nested for-loop join. It is very good for non-equi joins or coalescing joins."
      ],
      "metadata": {
        "id": "ddWXqe6i2bQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable broadcast Join and\n",
        "#Set Threshold limit of size in bytes of a dataFrame to broadcast\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
        "\n",
        "#Disable broadcast Join.\n",
        "#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "#Create a Larger DataFrame using weather Dataset in Databricks\n",
        "\n",
        "# Create DataFrames from sample data\n",
        "sales_data = [(1, 101, 2), (2, 102, 1), (3, 103, 3), (4, 101, 1), (5, 104, 4)]\n",
        "products_data = [(101, \"Learn C++\", 10), (102, \"Mobile: X1\", 20), (103, \"LCD\", 30), (104, \"Laptop\", 40)]\n",
        "\n",
        "sales_columns = [\"order_id\", \"product_id\", \"quantity\"]\n",
        "products_columns = [\"product_id\", \"product_name\", \"price\"]\n",
        "\n",
        "sales_df = spark.createDataFrame(sales_data, schema=sales_columns)\n",
        "products_df = spark.createDataFrame(products_data, schema=products_columns)\n",
        "\n",
        "# Perform broadcast join\n",
        "result = sales_df.join(broadcast(products_df), sales_df[\"product_id\"] == products_df[\"product_id\"])\n",
        "\n",
        "# Show result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbeAPRRd2aUZ",
        "outputId": "7fc079b2-2802-485e-f14a-964337c86bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+----------+------------+-----+\n",
            "|order_id|product_id|quantity|product_id|product_name|price|\n",
            "+--------+----------+--------+----------+------------+-----+\n",
            "|       1|       101|       2|       101|   Learn C++|   10|\n",
            "|       2|       102|       1|       102|  Mobile: X1|   20|\n",
            "|       3|       103|       3|       103|         LCD|   30|\n",
            "|       4|       101|       1|       101|   Learn C++|   10|\n",
            "|       5|       104|       4|       104|      Laptop|   40|\n",
            "+--------+----------+--------+----------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}